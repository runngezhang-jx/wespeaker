

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Runtime for Wespeaker &mdash; wespeaker 1.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=4c8675c2"></script>
      <script src="_static/doctools.js?v=888ff710"></script>
      <script src="_static/sphinx_highlight.js?v=4825356b"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Reference" href="reference.html" />
    <link rel="prev" title="Pretrained Models in Wespeaker" href="pretrained.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            wespeaker
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="python_package.html">Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">How to train models?</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained.html">Pretrained Models in Wespeaker</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Runtime for Wespeaker</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#platforms-supported">Platforms Supported</a></li>
<li class="toctree-l2"><a class="reference internal" href="#onnxruntime">Onnxruntime</a></li>
<li class="toctree-l2"><a class="reference internal" href="#server-tensorrt-gpu">Server (tensorrt gpu)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-0-train-a-model">Step 0. Train a model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-1-export-model">Step 1. Export model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#export-to-tensorrt-engine">Export to Tensorrt Engine</a></li>
<li class="toctree-l4"><a class="reference internal" href="#construct-model-repo">Construct Model Repo</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-build-server-and-start-server">Step 2. Build server and start server</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-build-client-and-start-client">Step 3. Build client and start client</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-test-score">Step 4. Test score</a></li>
<li class="toctree-l3"><a class="reference internal" href="#perf">Perf</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#pipeline-perf">Pipeline Perf</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribute.html">Contributing to Wespeaker</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">wespeaker</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Runtime for Wespeaker</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/runtime.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="runtime-for-wespeaker">
<h1>Runtime for Wespeaker<a class="headerlink" href="#runtime-for-wespeaker" title="Permalink to this heading"></a></h1>
<section id="platforms-supported">
<h2>Platforms Supported<a class="headerlink" href="#platforms-supported" title="Permalink to this heading"></a></h2>
<p>The Wespeaker runtime supports the following platforms.</p>
<ul class="simple">
<li><p>Server</p>
<ul>
<li><p><a class="reference external" href="https://github.com/wenet-e2e/wespeaker/tree/master/runtime/server/x86_gpu">TensorRT GPU</a></p></li>
</ul>
</li>
<li><p>Device</p>
<ul>
<li><p><a class="reference external" href="https://github.com/wenet-e2e/wespeaker/tree/master/runtime/onnxruntime">Onnxruntime</a></p>
<ul>
<li><p>linux_x86_cpu</p></li>
<li><p>linux_x86_gpu</p></li>
<li><p>macOS</p></li>
<li><p>windows</p></li>
</ul>
</li>
<li><p>Android (coming)</p></li>
<li><p>ncnn (coming)</p></li>
</ul>
</li>
</ul>
</section>
<section id="onnxruntime">
<h2>Onnxruntime<a class="headerlink" href="#onnxruntime" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>Step 1. Export your experiment model to ONNX by https://github.com/wenet-e2e/wespeaker/blob/master/wespeaker/bin/export_onnx.py</p></li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nv">exp</span><span class="o">=</span>exp<span class="w">  </span><span class="c1"># Change it to your experiment dir</span>
<span class="nv">onnx_dir</span><span class="o">=</span>onnx
python<span class="w"> </span>wespeaker/bin/export_onnx.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--config<span class="w"> </span><span class="nv">$exp</span>/config.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--checkpoint<span class="w"> </span><span class="nv">$exp</span>/avg_model.pt<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--output_model<span class="w"> </span><span class="nv">$onnx_dir</span>/final.onnx

<span class="c1"># When it finishes, you can find `final.onnx`.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Step 2. Build. The build requires cmake 3.14 or above, and gcc/g++ 5.4 or above.</p></li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>build<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
<span class="c1"># 1. no gpu</span>
cmake<span class="w"> </span>-DONNX<span class="o">=</span>ON<span class="w"> </span>..
<span class="c1"># 2. gpu (macOS don&#39;t supported)</span>
<span class="c1"># cmake -DONNX=ON -DGPU=ON ..</span>
cmake<span class="w"> </span>--build<span class="w"> </span>.
</pre></div>
</div>
<ul class="simple">
<li><p>Step 3. Testing.</p></li>
</ul>
<blockquote>
<div><p>NOTE: If using GPU, you need to specify the cuda path.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>/usr/local/cuda-11.1/bin<span class="si">${</span><span class="nv">PATH</span><span class="p">:+:</span><span class="si">${</span><span class="nv">PATH</span><span class="si">}}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda-11.1/lib64:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="p">:+:</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}}</span>
</pre></div>
</div>
</div></blockquote>
<ol class="simple">
<li><p>the RTF(real time factor) is shown in the console, and embedding will be written to the txt file.</p></li>
</ol>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">GLOG_logtostderr</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">GLOG_v</span><span class="o">=</span><span class="m">2</span>
<span class="nv">wav_scp</span><span class="o">=</span>your_test_wav_scp
<span class="nv">onnx_dir</span><span class="o">=</span>your_model_dir
<span class="nv">embed_out</span><span class="o">=</span>your_embedding_txt
./build/bin/extract_emb_main<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--wav_scp<span class="w"> </span><span class="nv">$wav_scp</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--result<span class="w"> </span><span class="nv">$embed_out</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--speaker_model_path<span class="w"> </span><span class="nv">$onnx_dir</span>/final.onnx<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--embedding_size<span class="w"> </span><span class="m">256</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--samples_per_chunk<span class="w">  </span><span class="m">32000</span><span class="w">  </span><span class="c1"># 2s</span>
</pre></div>
</div>
<blockquote>
<div><p>NOTE: samples_per_chunk: samples of one chunk. samples_per_chunk = sample_rate * duration</p>
<p>If samples_per_chunk = -1, compute the embedding of whole sentence;
else compute embedding with chunk by chunk, and then average embeddings of chunk.</p>
</div></blockquote>
<ol class="simple">
<li><p>Calculate the similarity of two speech.</p></li>
</ol>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">GLOG_logtostderr</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">GLOG_v</span><span class="o">=</span><span class="m">2</span>
<span class="nv">onnx_dir</span><span class="o">=</span>your_model_dir
./build/bin/asv_main<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--enroll_wav<span class="w"> </span>wav1_path<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--test_wav<span class="w"> </span>wav2_path<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--threshold<span class="w"> </span><span class="m">0</span>.5<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--speaker_model_path<span class="w"> </span><span class="nv">$onnx_dir</span>/final.onnx<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--embedding_size<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
</section>
<section id="server-tensorrt-gpu">
<h2>Server (tensorrt gpu)<a class="headerlink" href="#server-tensorrt-gpu" title="Permalink to this heading"></a></h2>
<section id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading"></a></h3>
<p>In this project, we use models trained in <a class="reference external" href="https://github.com/wenet-e2e/wespeaker">wespeaker</a> as an example to show how to convert speaker model to tensorrt and deploy them on <a class="reference external" href="https://github.com/triton-inference-server/server.git">Triton Inference Server</a>. If you only have CPUs, instead of using GPUs to deploy Tensorrt model, you may deploy the exported onnx model on Triton Inference Server as well.</p>
</section>
<section id="step-0-train-a-model">
<h3>Step 0. Train a model<a class="headerlink" href="#step-0-train-a-model" title="Permalink to this heading"></a></h3>
<p>Please follow wespeaker examples to train a model. After training, you should get several checkpoints under your <code class="docutils literal notranslate"><span class="pre">exp/xxx/models/</span></code> folder. We take <a class="reference external" href="https://github.com/wenet-e2e/wespeaker/tree/master/examples/voxceleb/v2">voxceleb</a> as an example.</p>
</section>
<section id="step-1-export-model">
<h3>Step 1. Export model<a class="headerlink" href="#step-1-export-model" title="Permalink to this heading"></a></h3>
<p>We’ll first export our model to onnx and then convert our onnx model to tensorrt.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># go to your example
cd wespeaker/examples/voxceleb/v2
. ./path.sh
exp_dir=exp/resnet
python3 wespeaker/bin/export_onnx.py --config=${exp_dir}/config.yaml --checkpoint=${exp_dir}/models/avg_model.pt --output_model=${exp_dir}/models/avg_model.onnx

# If you want to minus the mean vector in the onnx model, you may simply add the --mean_vec to the .npy mean vector file.
python3 wespeaker/bin/export_onnx.py --config=${exp_dir}/config.yaml --checkpoint=exp/resnet/models/avg_model.pt --output_model=exp/resnet/models/avg_model.onnx --mean_vec=${exp_dir}/embeddings/vox2_dev/mean_vec.npy
</pre></div>
</div>
<p>If you only want to deploy the onnx model on CPU or GPU, you may skip the Tensorrt part and go to <a class="reference internal" href="#construct-model-repo"><span class="std std-ref">the section</span></a> to construct your model repository.</p>
<section id="export-to-tensorrt-engine">
<h4>Export to Tensorrt Engine<a class="headerlink" href="#export-to-tensorrt-engine" title="Permalink to this heading"></a></h4>
<p>Now let’s convert our onnx model to tensorrt engine. We will deploy our model on Triton 22.03 therefore we here will use tensorrt 22.03 docker as an example to show how to convert the model. Please move your onnx model to the target platform/GPU you will deploy.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">run</span> <span class="o">--</span><span class="n">gpus</span> <span class="s1">&#39;&quot;device=0&quot;&#39;</span> <span class="o">-</span><span class="n">it</span> <span class="o">-</span><span class="n">v</span> <span class="o">&lt;</span><span class="n">the</span> <span class="n">output</span> <span class="n">onnx</span> <span class="n">model</span> <span class="n">directory</span><span class="o">&gt;</span><span class="p">:</span><span class="o">/</span><span class="n">models</span> <span class="n">nvcr</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">tensorrt</span><span class="p">:</span><span class="mf">22.03</span><span class="o">-</span><span class="n">py3</span>
<span class="n">cd</span> <span class="o">/</span><span class="n">models</span><span class="o">/</span>
<span class="c1"># shape=BxTxF  batchsize, sequence_length, feature_size</span>
<span class="n">trtexec</span> <span class="o">--</span><span class="n">saveEngine</span><span class="o">=</span><span class="n">b1_b128_s3000_fp16</span><span class="o">.</span><span class="n">trt</span>  <span class="o">--</span><span class="n">onnx</span><span class="o">=/</span><span class="n">models</span><span class="o">/</span><span class="n">avg_model</span><span class="o">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">minShapes</span><span class="o">=</span><span class="n">feats</span><span class="p">:</span><span class="mi">1</span><span class="n">x200x80</span> <span class="o">--</span><span class="n">optShapes</span><span class="o">=</span><span class="n">feats</span><span class="p">:</span><span class="mi">64</span><span class="n">x200x80</span> <span class="o">--</span><span class="n">maxShapes</span><span class="o">=</span><span class="n">feats</span><span class="p">:</span><span class="mi">128</span><span class="n">x3000x80</span> <span class="o">--</span><span class="n">fp16</span>
</pre></div>
</div>
<p>Here we get an engine which has maximum sequence length of 3000 and minimum length of 200. Since the frame stride is 10ms, 200 and 3000 corresponds to 2.02 seconds and 30.02 seconds respectively(kaldi feature extractor). Notice these numbers will differ and depend on your feature extractor parameters. Notice we’ve added <code class="docutils literal notranslate"><span class="pre">--fp16</span></code> and in pratice, we found this option will not affect the final accuracy and improve the perf at the same time.</p>
<p>You may set these numbers by your production requirements. If you only know the seconds of audio you will use and have no idea of how many frames it will generate, you may try the below script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torchaudio.compliance.kaldi</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">kaldi</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">audio_dur_in_seconds</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">feat_dim</span> <span class="o">=</span> <span class="mi">80</span>  <span class="c1"># please check config.yaml if you dont know</span>
<span class="n">sample_rate</span> <span class="o">=</span> <span class="mi">16000</span>

<span class="n">waveform</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sample_rate</span> <span class="o">*</span> <span class="n">audio_dur_in_seconds</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">feat_tensor</span> <span class="o">=</span> <span class="n">kaldi</span><span class="o">.</span><span class="n">fbank</span><span class="p">(</span><span class="n">waveform</span><span class="p">,</span>
                            <span class="n">num_mel_bins</span><span class="o">=</span><span class="n">feat_dim</span><span class="p">,</span>
                            <span class="n">frame_shift</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                            <span class="n">frame_length</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                            <span class="n">energy_floor</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                            <span class="n">window_type</span><span class="o">=</span><span class="s1">&#39;hamming&#39;</span><span class="p">,</span>
                            <span class="n">htk_compat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">use_energy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">dither</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feat_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (198, 80)</span>
</pre></div>
</div>
<p>Then you may find <code class="docutils literal notranslate"><span class="pre">198</span></code> is the actual number of frames for audio of 2 seconds long.</p>
<p>That’s it！We build an engine that can accept 2.02 to 30.02 seconds long audio. If your application can accept fixed audio segments, we suggest you to set the <code class="docutils literal notranslate"><span class="pre">minShapes</span></code>, <code class="docutils literal notranslate"><span class="pre">optShapes</span></code> and <code class="docutils literal notranslate"><span class="pre">maxShapes</span></code> to the same shape.</p>
</section>
<section id="construct-model-repo">
<h4>Construct Model Repo<a class="headerlink" href="#construct-model-repo" title="Permalink to this heading"></a></h4>
<p>Now edit the config file under <code class="docutils literal notranslate"><span class="pre">model_repo/speaker_model/config.pbtxt</span></code> and replace <code class="docutils literal notranslate"><span class="pre">default_model_filename:xxx</span></code> with the name of your engine (e.g., <code class="docutils literal notranslate"><span class="pre">b1_b128_s3000_fp16.trt</span></code>) or onnx model (e.g., <code class="docutils literal notranslate"><span class="pre">avg_model.onnx</span></code>) and put the engine or model under <code class="docutils literal notranslate"><span class="pre">model_repo/speaker_model/1/</span></code>.</p>
<p>And if you use other model settings or different model from ours (resnet34), for example, ecapa model, the embedding dim of which is 192, therefore, you should edit the <code class="docutils literal notranslate"><span class="pre">model_repo/speaker_model/config.pbtxt</span></code> and <code class="docutils literal notranslate"><span class="pre">model_repo/speaker/config.pbtxt</span></code> and set embedding dim to 192.</p>
<p>If your model is onnx model, you should also edit <code class="docutils literal notranslate"><span class="pre">backend:</span> <span class="pre">&quot;tensorrt&quot;</span></code> to <code class="docutils literal notranslate"><span class="pre">backend:</span> <span class="pre">&quot;onnxruntime&quot;</span></code> in <code class="docutils literal notranslate"><span class="pre">model_repo/speaker_model/config.pbtxt</span></code>.</p>
<p>If you want to deploy model on CPUs, you should edit <code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code> under <code class="docutils literal notranslate"><span class="pre">speaker</span></code> and <code class="docutils literal notranslate"><span class="pre">speaker_model</span></code> and replace <code class="docutils literal notranslate"><span class="pre">kind:</span> <span class="pre">KIND_GPU</span></code> to <code class="docutils literal notranslate"><span class="pre">kind:</span> <span class="pre">KIND_CPU</span></code>.</p>
<p>Notice Tensorrt engine can only run on GPUs.</p>
</section>
</section>
<section id="step-2-build-server-and-start-server">
<h3>Step 2. Build server and start server<a class="headerlink" href="#step-2-build-server-and-start-server" title="Permalink to this heading"></a></h3>
<p>Notice we use triton 22.03 in dockerfile. Be sure to use the triton that has the same version as your tensorrt.</p>
<p>Build server:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># server</span>
<span class="n">docker</span> <span class="n">build</span> <span class="o">.</span> <span class="o">-</span><span class="n">f</span> <span class="n">Dockerfile</span><span class="o">/</span><span class="n">dockerfile</span><span class="o">.</span><span class="n">server</span> <span class="o">-</span><span class="n">t</span> <span class="n">wespeaker</span><span class="p">:</span><span class="n">latest</span> <span class="o">--</span><span class="n">network</span> <span class="n">host</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker run --gpus &#39;&quot;device=0&quot;&#39; -v $PWD/model_repo:/ws/model_repo --shm-size=1g --ulimit memlock=-1 -p 8000:8000 -p 8001:8001 -p 8002:8002 --ulimit stack=67108864 -ti  wespeaker:latest
tritonserver --model-repository=/ws/model_repo
</pre></div>
</div>
<p>The port <code class="docutils literal notranslate"><span class="pre">8000</span></code> is for http request and <code class="docutils literal notranslate"><span class="pre">8001</span></code> for grpc request.</p>
</section>
<section id="step-3-build-client-and-start-client">
<h3>Step 3. Build client and start client<a class="headerlink" href="#step-3-build-client-and-start-client" title="Permalink to this heading"></a></h3>
<p>Build client:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># client</span>
<span class="n">docker</span> <span class="n">build</span> <span class="o">.</span> <span class="o">-</span><span class="n">f</span> <span class="n">Dockerfile</span><span class="o">/</span><span class="n">dockerfile</span><span class="o">.</span><span class="n">client</span> <span class="o">-</span><span class="n">t</span> <span class="n">wespeaker_client</span><span class="p">:</span><span class="n">latest</span> <span class="o">--</span><span class="n">network</span> <span class="n">host</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker run -it -v $PWD:/ws -v &lt;data path&gt;:&lt;data path&gt; --network=host wespeaker_client

# example command
cd /ws/client/
python3 client.py --url=&lt;ip of the server&gt;:8001 --wavscp=/raid/dgxsa/slyne/wespeaker/examples/voxceleb/v2/data/vox1/wav.scp --output_directory=&lt;to put the generated embeddings&gt;

# The output direcotry will be something like:
# xvector_000.ark xvextor_000.scp xvector_001.scp .....
</pre></div>
</div>
</section>
<section id="step-4-test-score">
<h3>Step 4. Test score<a class="headerlink" href="#step-4-test-score" title="Permalink to this heading"></a></h3>
<p>After you extract the embeddings, you can now use the same way as wespeaker to test these embeddings. For example, you may test the extracted embeddings in wespeaker by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>cat embeddings/xvector_*.scp &gt; embeddings/xvector.scp

config=conf/resnet.yaml
exp_dir=exp/resnet

mkdir -p embeddings/scores
trials_dir=data/vox1/trials
python -u wespeaker/bin/score.py \
    --exp_dir ${exp_dir} \
    --eval_scp_path /raid/dgxsa/slyne/wespeaker/runtime/server/x86_gpu/embeddings/xvector.scp \  # embeddings generated from our server
    --cal_mean True \
    --cal_mean_dir ${exp_dir}/embeddings/vox2_dev \
    --p_target 0.01 \
    --c_miss 1 \
    --c_fa 1 \
    ${trials_dir}/vox1_O_cleaned.kaldi ${trials_dir}/vox1_E_cleaned.kaldi ${trials_dir}/vox1_H_cleaned.kaldi \
    2&gt;&amp;1 | tee /raid/dgxsa/slyne/wespeaker/runtime/server/x86_gpu/embeddings/scores/vox1_cos_result
</pre></div>
</div>
</section>
<section id="perf">
<h3>Perf<a class="headerlink" href="#perf" title="Permalink to this heading"></a></h3>
<p>We build our engines for 2.02 seconds long audio only by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trtexec</span> <span class="o">--</span><span class="n">saveEngine</span><span class="o">=</span><span class="n">resnet_b1_b128_s200_fp16</span><span class="o">.</span><span class="n">trt</span>  <span class="o">--</span><span class="n">onnx</span><span class="o">=</span><span class="n">resnet</span><span class="o">/</span><span class="n">resnet_avg_model</span><span class="o">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">minShapes</span><span class="o">=</span><span class="n">feats</span><span class="p">:</span><span class="mi">1</span><span class="n">x200x80</span> <span class="o">--</span><span class="n">optShapes</span><span class="o">=</span><span class="n">feats</span><span class="p">:</span><span class="mi">64</span><span class="n">x200x80</span> <span class="o">--</span><span class="n">maxShapes</span><span class="o">=</span><span class="n">feats</span><span class="p">:</span><span class="mi">128</span><span class="n">x200x80</span> <span class="o">--</span><span class="n">fp16</span>

<span class="n">trtexec</span> <span class="o">--</span><span class="n">saveEngine</span><span class="o">=</span><span class="n">ecapa_b1_b128_s200_fp16</span><span class="o">.</span><span class="n">trt</span>  <span class="o">--</span><span class="n">onnx</span><span class="o">=</span><span class="n">ecapa</span><span class="o">/</span><span class="n">ecapa_avg_model</span><span class="o">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">minShapes</span><span class="o">=</span><span class="n">feats</span><span class="p">:</span><span class="mi">1</span><span class="n">x200x80</span> <span class="o">--</span><span class="n">optShapes</span><span class="o">=</span><span class="n">feats</span><span class="p">:</span><span class="mi">64</span><span class="n">x200x80</span> <span class="o">--</span><span class="n">maxShapes</span><span class="o">=</span><span class="n">feats</span><span class="p">:</span><span class="mi">128</span><span class="n">x200x80</span> <span class="o">--</span><span class="n">fp16</span>
</pre></div>
</div>
<ul class="simple">
<li><p>GPU: T4</p></li>
<li><p>resnet: resnet34.</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>Engine</th>
<th>Throughput (bz=64)</th>
<th>utter/s</th>
</tr>
</thead>
<tbody>
<tr>
<td>resnet_b1_b128_s200_fp16.trt</td>
<td>39.7842</td>
<td>2546</td>
</tr>
<tr>
<td>ecapa_b1_b128_s200_fp16.trt</td>
<td>52.958</td>
<td>3389</td>
</tr>
</tbody>
</table><section id="pipeline-perf">
<h4>Pipeline Perf<a class="headerlink" href="#pipeline-perf" title="Permalink to this heading"></a></h4>
<p>In client docker, we may test the whole pipeline performance.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">client</span><span class="o">/</span>
<span class="c1"># generate test input</span>
<span class="n">python3</span> <span class="n">generate_input</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">audio_file</span><span class="o">=</span><span class="n">test</span><span class="o">.</span><span class="n">wav</span> <span class="o">--</span><span class="n">seconds</span><span class="o">=</span><span class="mf">2.02</span>

<span class="n">perf_analyzer</span> <span class="o">-</span><span class="n">m</span> <span class="n">speaker</span> <span class="o">-</span><span class="n">b</span> <span class="mi">1</span> <span class="o">--</span><span class="n">concurrency</span><span class="o">-</span><span class="nb">range</span> <span class="mi">200</span><span class="p">:</span><span class="mi">1000</span><span class="p">:</span><span class="mi">200</span> <span class="o">--</span><span class="nb">input</span><span class="o">-</span><span class="n">data</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">u</span> <span class="n">localhost</span><span class="p">:</span><span class="mi">8000</span>
</pre></div>
</div>
<table border="1" class="docutils">
<thead>
<tr>
<th>Engine</th>
<th>Conccurency</th>
<th>Throughput</th>
<th>Avg Latency(ms)</th>
<th>P99 Latency(ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td>resnet_b1_b128_s200_fp16.trt</td>
<td>200</td>
<td>2033</td>
<td>98</td>
<td>111</td>
</tr>
<tr>
<td></td>
<td>400</td>
<td>2010</td>
<td>198</td>
<td>208</td>
</tr>
<tr>
<td>ecapa_b1_b128_s200_fp16.trt</td>
<td>200</td>
<td>2647</td>
<td>75</td>
<td>111</td>
</tr>
<tr>
<td></td>
<td>400</td>
<td>2726</td>
<td>147</td>
<td>172</td>
</tr>
</tbody>
</table></section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="pretrained.html" class="btn btn-neutral float-left" title="Pretrained Models in Wespeaker" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="reference.html" class="btn btn-neutral float-right" title="Reference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, wespeaker-team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>