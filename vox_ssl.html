

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SV Tutorial on VoxCeleb v3 (Self-Supervised) &mdash; wespeaker 1.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=4c8675c2"></script>
      <script src="_static/doctools.js?v=888ff710"></script>
      <script src="_static/sphinx_highlight.js?v=4825356b"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Diarization Tutorial on VoxConverse v2" href="voxconverse_diar.html" />
    <link rel="prev" title="SV Tutorial on VoxCeleb v2 (Supervised)" href="vox.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            wespeaker
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="python_package.html">Python Package</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="train.html">How to train models?</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="vox.html">SV Tutorial on VoxCeleb v2 (Supervised)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">SV Tutorial on VoxCeleb v3 (Self-Supervised)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#first-experiment">First Experiment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-1-download-data">Stage 1: Download Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-2-reformat-the-data">Stage 2: Reformat the Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-3-neural-network-training">Stage 3: Neural Network training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-4-speaker-embedding-extraction">Stage 4: Speaker Embedding Extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-5-scoring-the-evaluation-set">Stage 5: Scoring the Evaluation Set</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-6-optional-export-the-trained-model">Stage 6(Optional): Export the trained model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="voxconverse_diar.html">Diarization Tutorial on VoxConverse v2</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pretrained.html">Pretrained Models in Wespeaker</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Runtime for Wespeaker</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribute.html">Contributing to Wespeaker</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">wespeaker</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="train.html">How to train models?</a></li>
      <li class="breadcrumb-item active">SV Tutorial on VoxCeleb v3 (Self-Supervised)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/vox_ssl.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="sv-tutorial-on-voxceleb-v3-self-supervised">
<h1>SV Tutorial on VoxCeleb v3 (Self-Supervised)<a class="headerlink" href="#sv-tutorial-on-voxceleb-v3-self-supervised" title="Permalink to this heading"></a></h1>
<p>If you meet any problems when going through this tutorial, please feel free to ask in
github <a class="reference external" href="https://github.com/wenet-e2e/wespeaker/issues">issues</a>. Thanks for any kind of feedback.</p>
<section id="first-experiment">
<h2>First Experiment<a class="headerlink" href="#first-experiment" title="Permalink to this heading"></a></h2>
<p>We provide three self-supervised recipes on voxceleb data. They are currently the three most commonly used frameworks
for self-supervised speaker verification. If you want to learn more, you can refer to the <code class="docutils literal notranslate"><span class="pre">README.md</span></code> in the
corresponding directories.</p>
<ul class="simple">
<li><p>SimCLR: <code class="docutils literal notranslate"><span class="pre">examples/voxceleb/v3/simclr/run.sh</span></code></p></li>
<li><p>MoCo: <code class="docutils literal notranslate"><span class="pre">examples/voxceleb/v3/moco/run.sh</span></code></p></li>
<li><p>DINO: <code class="docutils literal notranslate"><span class="pre">examples/voxceleb/v3/dino/run.sh</span></code></p></li>
</ul>
<p>Because the steps of these three algorithms are basically the same, the following tutorial will take <strong>DINO</strong> as an
example. The recipe is simple and we suggest you run each stage one by one manually and check the result to understand
the whole processs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">examples</span><span class="o">/</span><span class="n">voxceleb</span><span class="o">/</span><span class="n">v3</span><span class="o">/</span><span class="n">dino</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">1</span> <span class="o">--</span><span class="n">stop_stage</span> <span class="mi">1</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">2</span> <span class="o">--</span><span class="n">stop_stage</span> <span class="mi">2</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">3</span> <span class="o">--</span><span class="n">stop_stage</span> <span class="mi">3</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">4</span> <span class="o">--</span><span class="n">stop_stage</span> <span class="mi">4</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">5</span> <span class="o">--</span><span class="n">stop_stage</span> <span class="mi">5</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">6</span> <span class="o">--</span><span class="n">stop_stage</span> <span class="mi">6</span>
</pre></div>
</div>
</section>
<section id="stage-1-download-data">
<h2>Stage 1: Download Data<a class="headerlink" href="#stage-1-download-data" title="Permalink to this heading"></a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>if [ ${stage} -le 1 ] &amp;&amp; [ ${stop_stage} -ge 1 ]; then
  echo &quot;Prepare datasets ...&quot;
  ./local/prepare_data.sh --stage 2 --stop_stage 4 --data ${data}
fi
</pre></div>
</div>
<p>This step is exactly the same as the recipe for supervised training on voxceleb <code class="docutils literal notranslate"><span class="pre">examples/voxceleb/v2</span></code>. <strong>If you have
done it before, you can skip this step.</strong></p>
<p>This stage prepares the <strong>voxceleb1</strong>, <strong>voxceleb2</strong>, <strong>MUSAN</strong> and <strong>RIRS_NOISES</strong> dataset. MUSAN is a noise dataset
and RIRS_NOISES is a reverberation dataset, which are used for data augmentation. It should be noted that for
self-supervised speaker verification, data augmentation is crucial for the training process. We strongly recommend
incorporating MUSAN and RIRS_NOISES data augmentation here.</p>
<p>It should be noted that the <code class="docutils literal notranslate"><span class="pre">./local/prepare_data.sh</span></code> script starts from the stage 2. It is because the data downloading
process in stage 1 will take a long time. Thus we recommand you to download all archives above in your own way first and
put it under <code class="docutils literal notranslate"><span class="pre">data/download_data</span></code> and then run the above script.</p>
<p>When finishing this stage, you will get the following meta files:</p>
<ul class="simple">
<li><p><strong>wav.scp</strong> files for all the dataset:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">data/musan/wav.scp</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data/rirs/wav.scp</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data/vox1/wav.scp</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data/vox2_dev/wav.scp</span></code></p></li>
</ul>
</li>
<li><p><strong>utt2spk</strong> and <strong>spk2utt</strong> files for voxceleb1 and voxceleb2_dev</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">data/vox1/utt2spk</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data/vox1/spk2utt</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data/vox2_dev/utt2spk</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data/vox2_dev/spk2utt</span></code></p></li>
</ul>
</li>
<li><p><strong>trials</strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">data/vox1/trials/vox1_O_cleaned.kaldi</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data/vox1/trials/vox1_E_cleaned.kaldi</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data/vox1/trials/vox1_H_cleaned.kaldi</span></code></p></li>
</ul>
</li>
</ul>
<p><strong>wav.scp</strong> each line records two blank-separated columns : <code class="docutils literal notranslate"><span class="pre">wav_id</span></code> and <code class="docutils literal notranslate"><span class="pre">wav_path</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">id10001</span><span class="o">/</span><span class="mi">1</span><span class="n">zcIwhmdeo4</span><span class="o">/</span><span class="mf">00001.</span><span class="n">wav</span> <span class="o">/</span><span class="n">exported</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">voxceleb1_wav_v2</span><span class="o">/</span><span class="n">id10001</span><span class="o">/</span><span class="mi">1</span><span class="n">zcIwhmdeo4</span><span class="o">/</span><span class="mf">00001.</span><span class="n">wav</span>
<span class="n">id10001</span><span class="o">/</span><span class="mi">1</span><span class="n">zcIwhmdeo4</span><span class="o">/</span><span class="mf">00002.</span><span class="n">wav</span> <span class="o">/</span><span class="n">exported</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">voxceleb1_wav_v2</span><span class="o">/</span><span class="n">id10001</span><span class="o">/</span><span class="mi">1</span><span class="n">zcIwhmdeo4</span><span class="o">/</span><span class="mf">00002.</span><span class="n">wav</span>
<span class="o">...</span>
</pre></div>
</div>
<p><strong>utt2spk</strong> each line records two blank-separated columns : <code class="docutils literal notranslate"><span class="pre">wav_id</span></code> and <code class="docutils literal notranslate"><span class="pre">spk_id</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">id10001</span><span class="o">/</span><span class="mi">1</span><span class="n">zcIwhmdeo4</span><span class="o">/</span><span class="mf">00001.</span><span class="n">wav</span> <span class="n">id10001</span>
<span class="n">id10001</span><span class="o">/</span><span class="mi">1</span><span class="n">zcIwhmdeo4</span><span class="o">/</span><span class="mf">00002.</span><span class="n">wav</span> <span class="n">id10001</span>
<span class="o">...</span>
</pre></div>
</div>
<p><strong>spk2utt</strong> each line records many blank-separated columns : <code class="docutils literal notranslate"><span class="pre">spk_id</span></code> and many <code class="docutils literal notranslate"><span class="pre">wav_id</span></code>s belong to this <code class="docutils literal notranslate"><span class="pre">spk_id</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">id10001</span> <span class="n">id10001</span><span class="o">/</span><span class="mi">1</span><span class="n">zcIwhmdeo4</span><span class="o">/</span><span class="mf">00001.</span><span class="n">wav</span> <span class="n">id10001</span><span class="o">/</span><span class="mi">1</span><span class="n">zcIwhmdeo4</span><span class="o">/</span><span class="mf">00002.</span><span class="n">wav</span> <span class="n">id10001</span><span class="o">/</span><span class="mi">1</span><span class="n">zcIwhmdeo4</span><span class="o">/</span><span class="mf">00003.</span><span class="n">wav</span> <span class="o">...</span>
<span class="n">id10002</span> <span class="n">id10002</span><span class="o">/</span><span class="mi">0</span><span class="n">_laIeN</span><span class="o">-</span><span class="n">Q44</span><span class="o">/</span><span class="mf">00001.</span><span class="n">wav</span> <span class="n">id10002</span><span class="o">/</span><span class="mi">6</span><span class="n">WO410QOeuo</span><span class="o">/</span><span class="mf">00001.</span><span class="n">wav</span> <span class="o">...</span>
<span class="o">...</span>
</pre></div>
</div>
<p><strong>trials</strong> each line records three blank-separated columns : <code class="docutils literal notranslate"><span class="pre">enroll_wav_id</span></code>, <code class="docutils literal notranslate"><span class="pre">test_wav_id</span></code> and <code class="docutils literal notranslate"><span class="pre">label</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">id10001</span><span class="o">/</span><span class="n">Y8hIVOBuels</span><span class="o">/</span><span class="mf">00001.</span><span class="n">wav</span> <span class="n">id10001</span><span class="o">/</span><span class="mi">1</span><span class="n">zcIwhmdeo4</span><span class="o">/</span><span class="mf">00001.</span><span class="n">wav</span> <span class="n">target</span>
<span class="n">id10001</span><span class="o">/</span><span class="n">Y8hIVOBuels</span><span class="o">/</span><span class="mf">00001.</span><span class="n">wav</span> <span class="n">id10943</span><span class="o">/</span><span class="n">vNCVj7yLWPU</span><span class="o">/</span><span class="mf">00005.</span><span class="n">wav</span> <span class="n">nontarget</span>
<span class="n">id10001</span><span class="o">/</span><span class="n">Y8hIVOBuels</span><span class="o">/</span><span class="mf">00001.</span><span class="n">wav</span> <span class="n">id10001</span><span class="o">/</span><span class="mi">7</span><span class="n">w0IBEWc9Qw</span><span class="o">/</span><span class="mf">00004.</span><span class="n">wav</span> <span class="n">target</span>
<span class="n">id10001</span><span class="o">/</span><span class="n">Y8hIVOBuels</span><span class="o">/</span><span class="mf">00001.</span><span class="n">wav</span> <span class="n">id10999</span><span class="o">/</span><span class="n">G5R2</span><span class="o">-</span><span class="n">Hl7YX8</span><span class="o">/</span><span class="mf">00008.</span><span class="n">wav</span> <span class="n">nontarget</span>
<span class="o">...</span>
</pre></div>
</div>
<p>In this step, we generated <strong>utt2spk</strong> and <strong>spk2utt</strong>, but we will not use any speaker labels during the training
process.</p>
</section>
<section id="stage-2-reformat-the-data">
<h2>Stage 2: Reformat the Data<a class="headerlink" href="#stage-2-reformat-the-data" title="Permalink to this heading"></a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>if [ ${stage} -le 2 ] &amp;&amp; [ ${stop_stage} -ge 2 ]; then
  echo &quot;Covert train and test data to ${data_type}...&quot;
  for dset in vox2_dev vox1; do
    if [ $data_type == &quot;shard&quot; ]; then
      python tools/make_shard_list.py --num_utts_per_shard 1000 \
          --num_threads 16 \
          --prefix shards \
          --shuffle \
          ${data}/$dset/wav.scp ${data}/$dset/utt2spk \
          ${data}/$dset/shards ${data}/$dset/shard.list
    else
      python tools/make_raw_list.py ${data}/$dset/wav.scp \
          ${data}/$dset/utt2spk ${data}/$dset/raw.list
    fi
  done
  # Convert all musan data to LMDB
  python tools/make_lmdb.py ${data}/musan/wav.scp ${data}/musan/lmdb
  # Convert all rirs data to LMDB
  python tools/make_lmdb.py ${data}/rirs/wav.scp ${data}/rirs/lmdb
fi
</pre></div>
</div>
<p>This step is exactly the same as the recipe for supervised training on voxceleb <code class="docutils literal notranslate"><span class="pre">examples/voxceleb/v2</span></code>. <strong>If you have
done it before, you can skip this step.</strong></p>
<p>The voxceleb dataset contains millions of wav files. Frequently opening the large scale small files will cause the IO
bottleneck. By default, the wav files from voxceleb dataset will be restored to some large binary shard files and the
shard files’ paths ared store in <code class="docutils literal notranslate"><span class="pre">$data/$dset/shard.list</span></code> file. In this script, the wav file number in each shard file
is set to <code class="docutils literal notranslate"><span class="pre">1000</span></code>.</p>
<p>Besides, the MUSAN and RIR_NOISES dataset are stored in LMDB format for fastly random-access in the training process.</p>
</section>
<section id="stage-3-neural-network-training">
<h2>Stage 3: Neural Network training<a class="headerlink" href="#stage-3-neural-network-training" title="Permalink to this heading"></a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>if [ ${stage} -le 3 ] &amp;&amp; [ ${stop_stage} -ge 3 ]; then
  echo &quot;Start training ...&quot;
  num_gpus=$(echo $gpus | awk -F &#39;,&#39; &#39;{print NF}&#39;)
  torchrun --standalone --nnodes=1 --nproc_per_node=$num_gpus \
    wespeaker/ssl/bin/train_dino.py --config $config \
      --exp_dir ${exp_dir} \
      --gpus $gpus \
      --num_avg ${num_avg} \
      --data_type &quot;${data_type}&quot; \
      --train_data ${data}/vox2_dev/${data_type}.list \
      --wav_scp ${data}/vox2_dev/wav.scp \
      --reverb_data ${data}/rirs/lmdb \
      --noise_data ${data}/musan/lmdb \
      ${checkpoint:+--checkpoint $checkpoint}
fi
</pre></div>
</div>
<p>The NN model is trained in this step.</p>
<ul class="simple">
<li><p>Multi-GPU mode</p></li>
</ul>
<p>Here, the <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> command is used to start <code class="docutils literal notranslate"><span class="pre">num_gpus</span></code> processes for pytorch DDP training. Set the gpus ids
using <code class="docutils literal notranslate"><span class="pre">gpus</span></code> local variable. For example, <code class="docutils literal notranslate"><span class="pre">gpus=&quot;[0,1]&quot;</span></code>, two gpus will be used and the used gpu idx is 0 and 1.</p>
<ul class="simple">
<li><p>Model Initialization</p></li>
</ul>
<p>By default, the model is randomly initialized. You can also use some pre-trained model’s weight to initialize the model
by specify the <code class="docutils literal notranslate"><span class="pre">model_init</span></code> param in the config file.</p>
<ul class="simple">
<li><p>Resume training</p></li>
</ul>
<p>If your experiment is terminated after running several epochs for some reasons (e.g. the GPU is accidentally used by
other people and is out-of-memory ), you could continue the training from a checkpoint model. Just find out the finished
epoch in <code class="docutils literal notranslate"><span class="pre">exp/your_exp/</span></code>, set <code class="docutils literal notranslate"><span class="pre">checkpoint=exp/your_exp/$n.pt</span></code> and run the <code class="docutils literal notranslate"><span class="pre">run.sh</span> <span class="pre">--stage</span> <span class="pre">3</span></code>. Then the training will
continue from the $n+1.pt</p>
<ul class="simple">
<li><p>Config</p></li>
</ul>
<p>The config of neural network structure, optimization parameter, loss parameters, and dataset can be set in a YAML format
file.</p>
<p>Besides, under <code class="docutils literal notranslate"><span class="pre">conf/</span></code>, we have provide the configuration for different models, like ecapa <code class="docutils literal notranslate"><span class="pre">conf/ecapa.yaml</span></code> and
resnet <code class="docutils literal notranslate"><span class="pre">conf/resnet34.yaml</span></code>.</p>
<ul class="simple">
<li><p>Self-supervised Training Related</p></li>
</ul>
<p>It’s should be noted that for MoCo and SimCLR, the python scripts should be <code class="docutils literal notranslate"><span class="pre">wespeaker/ssl/bin/train_contrastive.py</span></code>.</p>
<p>The biggest difference compared to supervised training recipe <code class="docutils literal notranslate"><span class="pre">examples/voxceleb/v2</span></code> is the way data is organized.
Because self supervised training cannot use real labels, it can only construct sample pairs for contrastive learning
through assumptions. (a) The segments cropped from the same utterance belong to the same speaker (b) The segments
cropped from different utterances belong to different speakers.</p>
<p>For self-suerpervised training recipes, <strong>dataloader</strong> is defined in <code class="docutils literal notranslate"><span class="pre">wespeaker/ssl/dataset/dataset.py</span></code>. Next, I will
briefly introduce the process of dataset.</p>
<p>Firstly, we define different data reading methods based on different data storage formats. And perform global and local
shuffling.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">DataList</span><span class="p">(</span><span class="n">lists</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">)</span>
<span class="k">if</span> <span class="n">data_type</span> <span class="o">==</span> <span class="s1">&#39;shard&#39;</span><span class="p">:</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">Processor</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">processor</span><span class="o">.</span><span class="n">url_opener</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">Processor</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">processor</span><span class="o">.</span><span class="n">tar_file_and_group</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">data_type</span> <span class="o">==</span> <span class="s1">&#39;raw&#39;</span><span class="p">:</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">Processor</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">processor</span><span class="o">.</span><span class="n">parse_raw</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">Processor</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">processor</span><span class="o">.</span><span class="n">parse_feat</span><span class="p">)</span>
<span class="c1"># Local shuffle</span>
<span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">Processor</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">processor</span><span class="o">.</span><span class="n">shuffle</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">configs</span><span class="p">[</span><span class="s1">&#39;shuffle_args&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>Then, we defined different sample pair composition methods for different training methods. For SimCLR and MoCo, we take
2 segments from each sentence randomly. For DINO, we will crop 2 short and 4 long segments to form a positive pair.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># random chunk</span>
<span class="n">frame_shift</span> <span class="o">=</span> <span class="n">configs</span><span class="p">[</span><span class="s1">&#39;fbank_args&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;frame_shift&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">frame_length</span> <span class="o">=</span> <span class="n">configs</span><span class="p">[</span><span class="s1">&#39;fbank_args&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;frame_length&#39;</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">chunk_info_args</span> <span class="o">=</span> <span class="n">configs</span><span class="p">[</span><span class="s1">&#39;chunk_info_args&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">chunk_info_args</span><span class="p">:</span>
    <span class="k">if</span> <span class="s1">&#39;chunk_len&#39;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
        <span class="n">chunk_info_args</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">chunk_info_args</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">frame_shift</span> <span class="o">+</span>
            <span class="n">frame_length</span><span class="p">)</span> <span class="o">*</span> <span class="n">resample_rate</span> <span class="o">//</span> <span class="mi">1000</span>
<span class="n">chunk_info_args</span><span class="p">[</span><span class="s1">&#39;data_type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_type</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">Processor</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">ssl_processor</span><span class="o">.</span><span class="n">random_chunk_for_dino</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">chunk_info_args</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, it is a very important data augmentation step. We will randomly apply different data augmentation strategies to
each segment here.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># add reverb &amp; noise</span>
<span class="n">aug_prob</span> <span class="o">=</span> <span class="n">configs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;aug_prob&#39;</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">)</span>
<span class="k">if</span> <span class="p">(</span><span class="n">reverb_lmdb_file</span> <span class="ow">and</span> <span class="n">noise_lmdb_file</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">aug_prob</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">):</span>
    <span class="n">reverb_data</span> <span class="o">=</span> <span class="n">LmdbData</span><span class="p">(</span><span class="n">reverb_lmdb_file</span><span class="p">)</span>
    <span class="n">noise_data</span> <span class="o">=</span> <span class="n">LmdbData</span><span class="p">(</span><span class="n">noise_lmdb_file</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">Processor</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">ssl_processor</span><span class="o">.</span><span class="n">add_reverb_noise</span><span class="p">,</span>
                        <span class="n">reverb_data</span><span class="p">,</span> <span class="n">noise_data</span><span class="p">,</span> <span class="n">resample_rate</span><span class="p">,</span>
                        <span class="n">aug_prob</span><span class="p">)</span>
</pre></div>
</div>
<p>Wespeaker notably facilitates effortless configuration for organizing diverse processors into a pipeline, ensuring both
efficiency and ease of extension. And the SSL related processors are defined in <code class="docutils literal notranslate"><span class="pre">wespeaker/ssl/dataset/processor.py</span></code></p>
<p>In addition, in order to be more compatible with the existing framework of WeSpeaker, we have added wrappers to the
training models of SimCLR, MoCo, and DINO, which are defined in <code class="docutils literal notranslate"><span class="pre">wespeaker/ssl/models</span></code>. It includes **additional modules
** required for SSL training, the definition of <strong>loss functions</strong> and so on.</p>
</section>
<section id="stage-4-speaker-embedding-extraction">
<h2>Stage 4: Speaker Embedding Extraction<a class="headerlink" href="#stage-4-speaker-embedding-extraction" title="Permalink to this heading"></a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>if [ ${stage} -le 4 ] &amp;&amp; [ ${stop_stage} -ge 4 ]; then
  echo &quot;Do model average ...&quot;
  avg_model=$exp_dir/models/avg_model.pt
  python wespeaker/ssl/bin/average_dino_model.py \
    --dst_model $avg_model \
    --src_path $exp_dir/models \
    --num ${num_avg}

  echo &quot;Extract embeddings ...&quot;
  local/extract_vox.sh \
    --exp_dir $exp_dir --model_path $avg_model \
    --nj 4 --gpus $gpus --data_type $data_type --data ${data}
fi
</pre></div>
</div>
<p>The embeddings for the train and evaluation set are extracted in this stage.</p>
<ul class="simple">
<li><p>Average Model</p></li>
</ul>
<p>Average the model’s weights from last <code class="docutils literal notranslate"><span class="pre">num_avg</span></code> checkpoints. This is a kind of model ensamble strategy to improve the
system performance.</p>
<p>It’s should be noted that for MoCo and SimCLR, the python scripts should
be <code class="docutils literal notranslate"><span class="pre">wespeaker/ssl/bin/average_contrastive_model.py</span></code>. Because self-supervised training frameworks generally require the
introduction of additional modules (such as student model, projection head et al.) to assist in training, it is
necessary to remove these additional modules in this step to facilitate subsequent feature extraction.</p>
<ul class="simple">
<li><p>Extract Embedding</p></li>
</ul>
<p>The extracted embeddings are stored in <code class="docutils literal notranslate"><span class="pre">exp/your_exp/embeddings</span></code> in kaldi scp,ark format. If there is something wrong
happened in this stage, you can check the log files under <code class="docutils literal notranslate"><span class="pre">exp/your_exp/embeddings/log</span></code> directory.</p>
</section>
<section id="stage-5-scoring-the-evaluation-set">
<h2>Stage 5: Scoring the Evaluation Set<a class="headerlink" href="#stage-5-scoring-the-evaluation-set" title="Permalink to this heading"></a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>if [ ${stage} -le 5 ] &amp;&amp; [ ${stop_stage} -ge 5 ]; then
  echo &quot;Score ...&quot;
  local/score.sh \
    --stage 1 --stop-stage 2 \
    --data ${data} \
    --exp_dir $exp_dir \
    --trials &quot;$trials&quot;
fi
</pre></div>
</div>
<p>All the trails listed in local variable <code class="docutils literal notranslate"><span class="pre">trials</span></code> is scored in this stage. <strong>Cosine similarity</strong> is used to calculate the
score for each trial pair. At the end of this stage, the Equal Error rate (EER), minDCF evaluation results are stored in
the <code class="docutils literal notranslate"><span class="pre">exp/your_exp/scores/vox1_cos_result</span></code> file. Besides, the detailed score for each trial with trial_name <code class="docutils literal notranslate"><span class="pre">trial_xx</span></code>
can be found in <code class="docutils literal notranslate"><span class="pre">exp/your_exp/scores/trial_xx.score</span></code> file.</p>
<p>Unlike supervised training recipe, we will not perform asnorm here because theoretically we cannot use any voxceleb
labels for score normalization.</p>
</section>
<section id="stage-6-optional-export-the-trained-model">
<h2>Stage 6(Optional): Export the trained model<a class="headerlink" href="#stage-6-optional-export-the-trained-model" title="Permalink to this heading"></a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>if [ ${stage} -le 6 ] &amp;&amp; [ ${stop_stage} -ge 6 ]; then
  echo &quot;Export the best model ...&quot;
  python wespeaker/bin/export_jit.py \
    --config $exp_dir/config.yaml \
    --checkpoint $exp_dir/models/avg_model.pt \
    --output_file $exp_dir/models/final.zip
fi
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">wenet/bin/export_jit.py</span></code> will export the trained model using Libtorch. The exported model files can be easily used for
C++ inference in our runtime.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="vox.html" class="btn btn-neutral float-left" title="SV Tutorial on VoxCeleb v2 (Supervised)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="voxconverse_diar.html" class="btn btn-neutral float-right" title="Diarization Tutorial on VoxConverse v2" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, wespeaker-team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>